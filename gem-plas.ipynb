{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13113306,"sourceType":"datasetVersion","datasetId":8306783},{"sourceId":13113319,"sourceType":"datasetVersion","datasetId":8306794},{"sourceId":13113655,"sourceType":"datasetVersion","datasetId":8306980},{"sourceId":13119103,"sourceType":"datasetVersion","datasetId":8310626},{"sourceId":13119266,"sourceType":"datasetVersion","datasetId":8310740}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\ncsv_file = \"/kaggle/input/sequence-cluster/sequence_clusters_with_reps.csv\"\nfasta_input_dir = \"/kaggle/input/blast-results/blast_results\"\n\nfasta_output_dir = \"/kaggle/working/fasta_output\"\nos.makedirs(fasta_output_dir, exist_ok=True)\n\ndf = pd.read_csv(csv_file)\n\nfor file in os.listdir(fasta_input_dir):\n    if file.endswith(\"_rep_hits.fasta\"):\n        src_path = os.path.join(fasta_input_dir, file)\n        dst_path = os.path.join(fasta_output_dir, file)\n        with open(src_path, \"r\") as src, open(dst_path, \"w\") as dst:\n            dst.write(src.read())\n\nfor _, row in df.iterrows():\n    cluster_id = row[\"cluster_id\"]\n    seq_id = row[\"sequence_id\"]\n    sequence = row[\"sequence\"]\n\n    fasta_file = os.path.join(fasta_output_dir, f\"cluster{cluster_id}_rep_hits.fasta\")\n\n    with open(fasta_file, \"a\") as f:\n        f.write(f\">{seq_id}\\n\")\n        f.write(f\"{sequence}\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom Bio import AlignIO\nfrom torch_geometric.data import Data\nimport random\nimport time\nfrom tqdm import tqdm\n\nfasta_dir = \"/kaggle/input/msa-output/msa_output1\"\ngraph_dir = \"/kaggle/working/graphs1\"\nos.makedirs(graph_dir, exist_ok=True)\n\ndef shannon_entropy(column):\n    freqs = [column.count(aa)/len(column) for aa in set(column) if aa != \"-\"]\n    return -sum(p*np.log2(p) for p in freqs if p > 0)\n\ndef mutual_information(col_i, col_j):\n    aa_set = list(set(col_i + col_j) - {\"-\"})\n    if not aa_set:\n        return 0.0\n    mi = 0.0\n    for a in aa_set:\n        for b in aa_set:\n            p_ab = sum((x==a and y==b) for x,y in zip(col_i,col_j)) / len(col_i)\n            p_a = col_i.count(a)/len(col_i)\n            p_b = col_j.count(b)/len(col_j)\n            if p_ab > 0:\n                mi += p_ab * np.log2(p_ab/(p_a*p_b))\n    return mi\n\nfiles = [f for f in os.listdir(fasta_dir) if f.endswith(\".fasta\")]\ntotal_files = len(files)\n\nprint(f\"Found {total_files} clusters to process\\n\")\n\nstart_all = time.time()\n\nfor idx, file in enumerate(files, 1):\n    cluster_name = file.replace(\".fasta\", \"\")\n    msa_fasta = os.path.join(fasta_dir, file)\n\n    print(f\"\\n[{idx}/{total_files}] Processing {cluster_name}...\")\n    start_time = time.time()\n\n    alignment = AlignIO.read(msa_fasta, \"fasta\")\n    L = alignment.get_alignment_length()\n    N = len(alignment)\n\n    node_features = []\n    for i in range(L):\n        col = [rec.seq[i] for rec in alignment]\n        entropy = shannon_entropy(col)\n        aa_counts = [col.count(aa)/N for aa in \"ACDEFGHIKLMNPQRSTVWY\"]\n        node_features.append([entropy] + aa_counts)\n    node_features = torch.tensor(node_features, dtype=torch.float)\n\n    edge_index, edge_attr = [], []\n\n    for i in range(L-1):\n        edge_index.append([i, i+1])\n        edge_index.append([i+1, i])\n        edge_attr.append([1.0])\n        edge_attr.append([1.0])\n\n    for i in tqdm(range(L), desc=f\"   MI edges for {cluster_name}\", leave=False):\n        for j in range(i+1, L):\n            mi = mutual_information(\n                [rec.seq[i] for rec in alignment],\n                [rec.seq[j] for rec in alignment]\n            )\n            if mi > 0.1:\n                edge_index.append([i, j])\n                edge_index.append([j, i])\n                edge_attr.append([mi])\n                edge_attr.append([mi])\n\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n\n    graph = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n    torch.save(graph, os.path.join(graph_dir, f\"{cluster_name}.pt\"))\n\n    elapsed = time.time() - start_time\n    avg_time = (time.time() - start_all) / idx\n    eta = avg_time * (total_files - idx)\n    print(f\"Done {cluster_name} | Time: {elapsed:.2f}s | ETA: {eta/60:.1f} min left\")\n\ntotal_time = time.time() - start_all\nprint(f\"\\n All clusters processed in {total_time/60:.2f} minutes\")\n\nsample_file = os.path.join(graph_dir, random.choice(os.listdir(graph_dir)))\nprint(f\"\\n Checking: {sample_file}\")\ng = torch.load(sample_file)\nprint(\"x:\", g.x.shape)\nprint(\"edge_index:\", g.edge_index.shape)\nprint(\"edge_attr:\", g.edge_attr.shape)\nprint(g)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport random\nimport torch_geometric\ngraph_path = \"/kaggle/input/graphs/graphs1/cluster125_rep_hits_aligned.pt\"\ng = torch.load(graph_path, weights_only=False)\n\nprint(g)\n\nG = nx.Graph()\nnum_nodes = g.x.shape[0]\n\nentropies = g.x[:, 0].numpy()\n\nfor i in range(num_nodes):\n    G.add_node(i, entropy=entropies[i])\n\nedge_index = g.edge_index.t().numpy()\nedge_attr = g.edge_attr.numpy()\n\nbackbone_edges = [(u, v) for (u, v), w in zip(edge_index, edge_attr) if w == 1.0]\nmi_edges = [(u, v) for (u, v), w in zip(edge_index, edge_attr) if w > 1.0]\n\nsampled_mi_edges = random.sample(mi_edges,len(mi_edges))\n\nG.add_edges_from(backbone_edges, kind=\"backbone\")\nG.add_edges_from(sampled_mi_edges, kind=\"mi\")\n\n\npos = nx.spring_layout(G, seed=42, k=0.2)\n\n\nplt.figure(figsize=(10, 10))\n\nnx.draw_networkx_nodes(\n    G, pos,\n    node_size=50,\n    node_color=[G.nodes[i][\"entropy\"] for i in G.nodes()],\n    cmap=plt.cm.viridis\n)\n\nnx.draw_networkx_edges(G, pos, edgelist=backbone_edges, edge_color=\"gray\", alpha=0.3)\nnx.draw_networkx_edges(G, pos, edgelist=sampled_mi_edges, edge_color=\"red\", alpha=0.4)\n\nplt.title(\"Cluster12 Graph (Backbone + sampled MI edges)\")\nplt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis),\n             ax=plt.gca(), label=\"Shannon entropy\")\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ncsv_clusters = \"/kaggle/input/sequence-cluster/sequence_clusters_with_reps.csv\"\ncsv_labels = \"/kaggle/input/7enzymes/enzymes_plastics_selected7.csv\"\n\ndf_clusters = pd.read_csv(csv_clusters)\ndf_labels = pd.read_csv(csv_labels)\n\ndef clean_seq(s):\n    return str(s).replace(\"-\", \"\").replace(\"\\n\", \"\").upper()\n\ndf_clusters[\"sequence_clean\"] = df_clusters[\"sequence\"].apply(clean_seq)\ndf_labels[\"sequence_clean\"] = df_labels[\"sequence\"].apply(clean_seq)\n\ndf_merged = df_clusters.merge(df_labels, on=\"sequence_clean\", how=\"inner\")\n\nlabel_cols = [\"PET\", \"PHB\", \"PHA\", \"PLA\", \"PCL\", \"PU/PUR\", \"NYLON/PA\"]\n\ndef majority_label(subdf):\n    sums = subdf[label_cols].sum()\n    if sums.sum() == 0:\n        return \"Unknown\"\n    return sums.idxmax()\n\ncluster_to_label = df_merged.groupby(\"cluster_id\").apply(majority_label).to_dict()\n\ndf_cluster_labels = pd.DataFrame(list(cluster_to_label.items()), columns=[\"cluster_id\", \"enzyme_label\"])\ndf_cluster_labels.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cluster_labels[\"file\"] = df_cluster_labels[\"cluster_id\"].apply(\n    lambda cid: f\"cluster{cid}_rep_hits_aligned.pt\"\n)\ndf_cluster_labels.to_csv(\"/kaggle/working/index.csv\")\nprint(df_cluster_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, AutoModel\n\ngraph_dir = \"/kaggle/input/graphs/graphs1\"\nlabel_csv = \"/kaggle/working/index.csv\"\ncluster_csv = \"/kaggle/input/sequence-cluster/sequence_clusters_with_reps.csv\"\n\ndf_labels = pd.read_csv(label_csv)\nle = LabelEncoder()\ndf_labels[\"label_id\"] = le.fit_transform(df_labels[\"enzyme_label\"])\n\n\nesm_model_name = \"facebook/esm2_t33_650M_UR50D\"\ntokenizer = AutoTokenizer.from_pretrained(esm_model_name)\nesm_model = AutoModel.from_pretrained(esm_model_name).cuda()\n\ndf_clusters = pd.read_csv(cluster_csv)\nrep_sequences = df_clusters[df_clusters[\"is_representative\"] == 1][[\"cluster_id\", \"sequence\"]]\n\ncluster2esm = {}\ndef get_esm_embedding(seq):\n    inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1024)\n    inputs = {k: v.cuda() for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = esm_model(**inputs)\n        hidden = outputs.last_hidden_state.squeeze(0)  # [L, 1280]\n        emb = hidden.mean(dim=0)  \n    return emb.cpu()\n\nfor _, row in rep_sequences.iterrows():\n    cid = row[\"cluster_id\"]\n    seq = row[\"sequence\"].replace(\"-\", \"\").upper()\n    cluster2esm[cid] = get_esm_embedding(seq)\n\n\ngraphs, labels, esm_embeddings = [], [], []\n\nfor _, row in df_labels.iterrows():\n    file_path = os.path.join(graph_dir, row[\"file\"])\n    cluster_id = row[\"cluster_id\"]\n    if os.path.exists(file_path) and cluster_id in cluster2esm:\n        g = torch.load(file_path, weights_only=False)\n        g.y = torch.tensor([row[\"label_id\"]], dtype=torch.long)\n        graphs.append(g)\n        labels.append(row[\"label_id\"])\n        esm_embeddings.append(cluster2esm[cluster_id])  \n\nesm_embeddings = torch.stack(esm_embeddings)\n\n\ntrain_idx, test_idx = train_test_split(\n    range(len(graphs)), test_size=0.3, stratify=labels, random_state=42\n)\ntrain_dataset = [(graphs[i], esm_embeddings[i], labels[i]) for i in train_idx]\ntest_dataset = [(graphs[i], esm_embeddings[i], labels[i]) for i in test_idx]\n\nclass GraphSeqDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    def __len__(self):\n        return len(self.dataset)\n    def __getitem__(self, idx):\n        g, esm, label = self.dataset[idx]\n        return g, esm, label\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\nclass GNNOnly(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = nn.Linear(hidden_channels, num_classes)\n    def forward(self, g):\n        x, edge_index, batch = g.x, g.edge_index, g.batch\n        x = F.relu(self.conv1(x, edge_index))\n        x = F.relu(self.conv2(x, edge_index))\n        gnn_out = global_mean_pool(x, batch)\n        return self.lin(gnn_out)\n\nclass ESMOnly(nn.Module):\n    def __init__(self, esm_dim, num_classes):\n        super().__init__()\n        self.lin1 = nn.Linear(esm_dim, 512)\n        self.lin2 = nn.Linear(512, num_classes)\n    def forward(self, esm_emb):\n        x = F.relu(self.lin1(esm_emb))\n        return self.lin2(x)\n\nclass GNN_ESM(nn.Module):\n    def __init__(self, in_channels, hidden_channels, esm_dim, num_classes):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = nn.Linear(hidden_channels + esm_dim, num_classes)\n    def forward(self, g, esm_emb):\n        x, edge_index, batch = g.x, g.edge_index, g.batch\n        x = F.relu(self.conv1(x, edge_index))\n        x = F.relu(self.conv2(x, edge_index))\n        gnn_out = global_mean_pool(x, batch)\n        fusion = torch.cat([gnn_out, esm_emb], dim=1)\n        return self.lin(fusion)\n\ndef train_model(model, optimizer, criterion, mode=\"fusion\"):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for g, esm, label in train_loader:\n        g, esm, label = g.to(device), esm.to(device), torch.tensor(label).to(device)\n        optimizer.zero_grad()\n        if mode == \"gnn\":\n            out = model(g)\n        elif mode == \"esm\":\n            out = model(esm)\n        else:\n            out = model(g, esm)\n        loss = criterion(out, label)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pred = out.argmax(dim=1)\n        correct += (pred == label).sum().item()\n        total += label.size(0)\n    return total_loss / len(train_loader), correct / total\n\ndef eval_model(model, mode=\"fusion\"):\n    model.eval()\n    preds, trues = [], []\n    with torch.no_grad():\n        for g, esm, label in test_loader:\n            g, esm, label = g.to(device), esm.to(device), torch.tensor(label).to(device)\n            if mode == \"gnn\":\n                out = model(g)\n            elif mode == \"esm\":\n                out = model(esm)\n            else:\n                out = model(g, esm)\n            pred = out.argmax(dim=1).cpu().numpy()\n            preds.extend(pred)\n            trues.extend(label.cpu().numpy())\n    acc = (torch.tensor(preds) == torch.tensor(trues)).float().mean().item()\n    return acc, preds, trues\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.CrossEntropyLoss()\n\nconfigs = {\n    \"GNN-only\": (GNNOnly(graphs[0].x.shape[1], 128, len(le.classes_)), \"gnn\"),\n    \"ESM-only\": (ESMOnly(esm_embeddings.shape[1], len(le.classes_)), \"esm\"),\n    \"Fusion\": (GNN_ESM(graphs[0].x.shape[1], 128, esm_embeddings.shape[1], len(le.classes_)), \"fusion\")\n}\n\nfor name, (model, mode) in configs.items():\n    print(f\"\\n=== {name} ===\")\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n\n    for epoch in range(1, 51):\n        loss, train_acc = train_model(model, optimizer, criterion, mode)\n        test_acc, preds, trues = eval_model(model, mode)\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n\n    print(\"\\n--- Classification Report ---\")\n    print(classification_report(trues, preds, target_names=le.classes_))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.CrossEntropyLoss()\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nconfigs = {\n    \"GNN-only\": (GNNOnly(graphs[0].x.shape[1], 128, len(le.classes_)), \"gnn\"),\n    \"ESM-only\": (ESMOnly(esm_embeddings.shape[1], len(le.classes_)), \"esm\"),\n    \"Fusion\": (GNN_ESM(graphs[0].x.shape[1], 128, esm_embeddings.shape[1], len(le.classes_)), \"fusion\")\n}\n\nfor name, (model_template, mode) in configs.items():\n    print(f\"\\n=== {name} | 5-Fold CV ===\")\n    fold_accs, fold_reports = [], []\n\n    for fold, (train_idx, test_idx) in enumerate(skf.split(graphs, labels), 1):\n        print(f\"\\n--- Fold {fold} ---\")\n\n        train_dataset = [(graphs[i], esm_embeddings[i], labels[i]) for i in train_idx]\n        test_dataset  = [(graphs[i], esm_embeddings[i], labels[i]) for i in test_idx]\n        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n        test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False)\n\n        if mode == \"gnn\":\n            model = GNNOnly(graphs[0].x.shape[1], 128, len(le.classes_)).to(device)\n        elif mode == \"esm\":\n            model = ESMOnly(esm_embeddings.shape[1], len(le.classes_)).to(device)\n        else:\n            model = GNN_ESM(graphs[0].x.shape[1], 128, esm_embeddings.shape[1], len(le.classes_)).to(device)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n\n        for epoch in range(1, 31): \n            loss, train_acc = train_model(model, optimizer, criterion, mode)\n            test_acc, preds, trues = eval_model(model, mode)\n            if epoch % 10 == 0:\n                print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n\n        fold_accs.append(test_acc)\n        fold_reports.append(classification_report(trues, preds, target_names=le.classes_, output_dict=True))\n\n\n    print(f\" Test Acc: {np.mean(fold_accs):.4f} Â± {np.std(fold_accs):.4f}\")\n\n    avg_report = {}\n    for label in le.classes_:\n        precisions = [r[label][\"precision\"] for r in fold_reports if label in r]\n        recalls = [r[label][\"recall\"] for r in fold_reports if label in r]\n        f1s = [r[label][\"f1-score\"] for r in fold_reports if label in r]\n        avg_report[label] = {\n            \"precision\": np.mean(precisions),\n            \"recall\": np.mean(recalls),\n            \"f1-score\": np.mean(f1s)\n        }\n\n\n    for label, scores in avg_report.items():\n        print(f\"{label:10s} | P: {scores['precision']:.3f}, R: {scores['recall']:.3f}, F1: {scores['f1-score']:.3f}\")\n","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}